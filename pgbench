#!/usr/bin/env python3
#
# Copyright (c) 2016 MagicStack Inc.
# All rights reserved.
#
# See LICENSE for details.
##


import argparse
import collections
import datetime
import glob
import html
import json
import math
import os.path
import platform
import re
import string
import subprocess
import sys
import warnings

import asyncpg.cluster
import numpy as np


def platform_info():
    machine = platform.machine()
    processor = platform.processor()
    system = platform.system()

    cpuinfo_f = '/proc/cpuinfo'

    if (processor in {machine, 'unknown'} and os.path.exists(cpuinfo_f)):
        with open(cpuinfo_f, 'rt') as f:
            for line in f:
                if line.startswith('model name'):
                    _, _, p = line.partition(':')
                    processor = p.strip()
                    break

    if 'Linux' in system:

        with warnings.catch_warnings():
            # see issue #1322 for more information
            warnings.filterwarnings(
                'ignore',
                'dist\(\) and linux_distribution\(\) '
                'functions are deprecated .*',
                PendingDeprecationWarning,
            )
            distname, distversion, distid = platform.dist('')

        distribution = '{} {}'.format(distname, distversion).strip()

    else:
        distribution = None

    data = {
        'cpu': processor,
        'arch': machine,
        'system': '{} {}'.format(system, platform.release()),
        'distribution': distribution
    }

    return data


def weighted_quantile(values, quantiles, weights):
    """Very close to np.percentile, but supports weights.

    :param values: np.array with data
    :param quantiles: array-like with many quantiles needed,
           quantiles should be in [0, 1]!
    :param weights: array-like of the same length as `array`
    :return: np.array with computed quantiles.
    """
    values = np.array(values)
    quantiles = np.array(quantiles)
    weights = np.array(weights)
    if not (np.all(quantiles >= 0) and np.all(quantiles <= 1)):
        raise ValueError('quantiles should be in [0, 1]')

    weighted_quantiles = np.cumsum(weights) - 0.5 * weights
    weighted_quantiles -= weighted_quantiles[0]
    weighted_quantiles /= weighted_quantiles[-1]

    return np.interp(quantiles, weighted_quantiles, values)


percentiles = [25, 50, 75, 90, 99, 99.99]


def calc_latency_stats(queries, rows, duration, min_latency, max_latency,
                       latency_stats, output_format='text'):
    arange = np.arange(len(latency_stats))

    mean_latency = np.average(arange, weights=latency_stats)
    variance = np.average((arange - mean_latency) ** 2, weights=latency_stats)
    latency_std = math.sqrt(variance)
    latency_cv = latency_std / mean_latency

    percentile_data = []

    quantiles = weighted_quantile(arange, [p / 100 for p in percentiles],
                                  weights=latency_stats)

    for i, percentile in enumerate(percentiles):
        percentile_data.append((percentile, round(quantiles[i] / 100, 3)))

    data = dict(
        duration=round(duration, 2),
        queries=queries,
        qps=round(queries / duration, 2),
        rps=round(rows / duration, 2),
        latency_min=round(min_latency / 100, 3),
        latency_mean=round(mean_latency / 100, 3),
        latency_max=round(max_latency / 100, 3),
        latency_std=round(latency_std / 100, 3),
        latency_cv=round(latency_cv * 100, 2),
        latency_percentiles=percentile_data
    )

    return data


def _geom_mean(values):
    p = 1
    root = 0
    for val in values:
        p *= val
        root += 1

    return p ** (1.0 / root)


def mean_latency_stats(data):
    data = dict(
        duration=round(_geom_mean(d['duration'] for d in data), 2),
        queries=round(_geom_mean(d['queries'] for d in data), 2),
        qps=round(_geom_mean(d['qps'] for d in data), 2),
        rps=round(_geom_mean(d['rps'] for d in data), 2),
        latency_min=round(_geom_mean(d['latency_min'] for d in data), 3),
        latency_mean=round(_geom_mean(d['latency_mean'] for d in data), 3),
        latency_max=round(_geom_mean(d['latency_max'] for d in data), 3),
        latency_std=round(_geom_mean(d['latency_std'] for d in data), 3),
        latency_cv=round(_geom_mean(d['latency_cv'] for d in data), 2),
        latency_percentiles=[
            (p, round(_geom_mean(d['latency_percentiles'][i][1] for d in data),
                      3))
            for i, p in enumerate(percentiles)
        ]
    )

    return data


def format_text(data):
    data = dict(data)

    data['latency_percentiles'] = '; '.join(
        '{}% under {}ms'.format(*v) for v in data['latency_percentiles'])

    output = '''\
{queries} queries in {duration} seconds
Latency: min {latency_min}ms; max {latency_max}ms; mean {latency_mean}ms; \
std: {latency_std}ms ({latency_cv}%)
Latency distribution: {latency_percentiles}
Queries/sec: {qps}
Rows/sec: {rps}
'''.format(**data)

    return output


def process_results(results):
    try:
        lat_data = json.loads(results)
    except json.JSONDecodeError as e:
        print('could not process benchmark results: {}'.format(e),
              file=sys.stderr)
        print(results, file=sys.stderr)
        sys.exit(1)

    latency_stats = np.array(lat_data['latency_stats'])

    return calc_latency_stats(
        lat_data['queries'], lat_data['rows'], lat_data['duration'],
        lat_data['min_latency'], lat_data['max_latency'], latency_stats)


def format_report(data, target_file):
    tpl_path = os.path.join(os.path.dirname(__file__), 'report', 'report.html')

    with open(tpl_path, 'r') as f:
        tpl = string.Template(f.read())

    now = datetime.datetime.now()
    date = now.strftime('%c')
    platform = '{system} ({dist}, {arch}) on {cpu}'.format(
        system=data['platform']['system'],
        dist=data['platform']['distribution'],
        arch=data['platform']['arch'],
        cpu=data['platform']['cpu'],
    )

    i = 0

    entries = collections.OrderedDict()

    for benchmark in data['benchmarks']:
        entry = {}

        bname = benchmark['name']

        try:
            entry = entries[bname]
        except KeyError:
            entry = entries[bname] = {
                'name': bname,
                'benchmarks': collections.OrderedDict((
                    ('Queries/sec', []),
                    ('Rows/sec', []),
                    ('Min latency', []),
                    ('Mean latency', []),
                    ('Max latency', []),
                    ('Latency variation', []),
                ))
            }

        brecords = entry['benchmarks']
        variations = benchmark['variations']

        bmean = benchmark['mean']

        brecords['Queries/sec'].append(
            bmean['qps'])
        brecords['Rows/sec'].append(
            '{}'.format(bmean['rps']))
        brecords['Min latency'].append(
            '{}ms'.format(bmean['latency_min']))
        brecords['Mean latency'].append(
            '{}ms'.format(bmean['latency_mean']))
        brecords['Max latency'].append(
            '{}ms'.format(bmean['latency_max']))
        brecords['Latency variation'].append('{}ms ({}%)'.format(
            bmean['latency_std'], bmean['latency_cv']))

        i = 0
        for concurrency in data['concurrency_levels']:
            for query in data['querynames']:
                variation = variations[i]
                i += 1

                brecords['Queries/sec'].append(
                    variation['qps'])
                brecords['Rows/sec'].append(
                    '{}'.format(variation['rps']))
                brecords['Min latency'].append(
                    '{}ms'.format(variation['latency_min']))
                brecords['Mean latency'].append(
                    '{}ms'.format(variation['latency_mean']))
                brecords['Max latency'].append(
                    '{}ms'.format(variation['latency_max']))
                brecords['Latency variation'].append('{}ms ({}%)'.format(
                    variation['latency_std'], variation['latency_cv']))

    vc = len(data['concurrency_levels']) * len(data['querynames'])

    variations_th = ['<th>Geometric mean</th>']
    query_content = []
    nqueries = len(data['querynames'])

    for i, concurrency in enumerate(data['concurrency_levels']):
        for j, queryname in enumerate(data['querynames']):
            variations_th.append(
                '<th>{}</th>'.format(
                    '{} x{}'.format(queryname, concurrency)
                )
            )

            query_data = data['queries'][j]

            query_content.append(
                '''
                    <h3>{n} x{c}</h3>
                    <h4>Test Setup</h4>
                    <pre style="white-space: pre-wrap;
                                margin-bottom: 10px;
                                margin-left: 40px;">{setupquery}</pre>
                    <h4>Test Query</h4>
                    <pre style="white-space: pre-wrap;
                                margin-bottom: 10px;
                                margin-left: 40px;">{query}</pre>
                    <svg id="query-bars-{id}" style="width: 80vw"></svg>
                    <svg id="query-lats-{id}" style="width: 80vw"></svg>
                '''.format(n=queryname, c=concurrency, id=i * nqueries + j,
                           query=html.escape(query_data['query']),
                           setupquery=html.escape(
                               query_data.get('setup', 'N/A')))
            )

    record_trs = []
    for bname, entry in entries.items():
        record_trs.append(
            '''<tr class="benchmark">
                <td>{name}</td>
                {empty_tds}
            </tr>'''.format(name=bname, empty_tds='<td></td>' * vc)
        )

        for metric, metric_data in entry['benchmarks'].items():
            record_trs.append(
                '<tr class="metric"><td>{metric}</td>{data}</tr>'.format(
                    metric=metric,
                    data='\n'.join('<td>{}</td>'.format(v)
                                   for v in metric_data)
                )
            )

    table = '''
        <table class="results">
            <thead>
                <tr>
                    <th></th>
                    {variations_header}
                </tr>
            </thead>
            <tbody>
                {records}
            </tbody>
        </table>
    '''.format(variations_header='\n'.join(variations_th),
               records='\n'.join(record_trs))

    output = tpl.safe_substitute(
        __BENCHMARK_DATE__=date,
        __BENCHMARK_PLATFORM__=platform,
        __BENCHMARK_DATA_TABLE__=table,
        __BENCHMARK_DATA_JSON__=json.dumps(data),
        __BENCHMARK_QUERY_CONTENT__='\n'.join(query_content)
    )

    with open(target_file, 'wt') as f:
        f.write(output)


BENCHMARKS = [
    'golang-libpq',
    'golang-pgx',
    'python-aiopg',
    'python-asyncpg',
    'python-psycopg',
    'python-postgresql',
    'nodejs-pg',
    'nodejs-pg-native',
]


def run_benchmarks(args, variations, pghost, pgport, pguser):
    if args.benchmark:
        benchmarks_to_run = [re.compile(b) for b in args.benchmark]
    else:
        benchmarks_to_run = [re.compile(re.escape(b)) for b in BENCHMARKS]

    mydir = os.path.dirname(os.path.realpath(__file__))

    benchmarks_data = []

    for benchmark in BENCHMARKS:
        if not any(b.match(benchmark) for b in benchmarks_to_run):
            continue

        msg = 'Running {} benchmarks...'.format(benchmark)
        print(msg)
        print('=' * len(msg))
        print()

        platform, _, driver = benchmark.partition('-')
        runner = os.path.join(mydir, 'pgbench_{}'.format(platform))

        runner_args = {
            'warmup-time': args.warmup_time,
            'duration': args.duration,
            'timeout': args.timeout,
            'pghost': pghost,
            'pgport': pgport,
            'pguser': pguser,
            'output-format': 'json'
        }

        runner_switches = ['--{}={}'.format(k.replace('_', '-'), v)
                           for k, v in runner_args.items()]

        benchmark_data = {
            'name': benchmark,
            'variations': [],
            'mean': {}
        }

        benchmarks_data.append(benchmark_data)

        for variation in variations:
            print(variation['title'])
            print('-' * len(variation['title']))
            print()
            cmdline = [runner] + runner_switches + \
                      ['--concurrency={}'.format(variation['concurrency'])] + \
                      [driver, variation['queryfile']]
            print(' '.join(cmdline))

            runner_proc = subprocess.run(
                cmdline, stdout=subprocess.PIPE, stderr=sys.stderr)

            if runner_proc.returncode != 0:
                msg = 'fatal: benchmark runner exited with exit ' \
                      'code {}'.format(runner_proc.returncode)
                print(msg)
                sys.exit(runner_proc.returncode)

            data = process_results(runner_proc.stdout.decode('utf-8'))
            benchmark_data['variations'].append(data)

            print(format_text(data))

        mean = mean_latency_stats(benchmark_data['variations'])
        benchmark_data['mean'] = mean

        if len(variations) > 1:
            msg = 'Geometric mean results'
            print(msg)
            print('-' * len(msg))
            print()
            print(format_text(mean))

    return benchmarks_data


def main():
    parser = argparse.ArgumentParser(
        description='pg driver implementation benchmark runner')
    parser.add_argument(
        '--concurrency-levels', type=str, default='10',
        help='comma-separated concurrency-levels')
    parser.add_argument(
        '--warmup-time', type=int, default=5,
        help='duration of warmup period for each benchmark in seconds')
    parser.add_argument(
        '--duration', type=int, default=30,
        help='duration of each benchmark in seconds')
    parser.add_argument(
        '--timeout', default=2, type=int,
        help='server timeout in seconds')
    parser.add_argument(
        '--save-json', '-J', type=str,
        help='path to save benchmark results in JSON format')
    parser.add_argument(
        '--save-html', '-H', type=str,
        help='path to save benchmark results in HTML format')
    parser.add_argument(
        '--pghost', type=str,
        help='PostgreSQL server host.  If not specified, ' +
             'a temporary cluster will be used.')
    parser.add_argument(
        '--pgport', type=int, default=5432,
        help='PostgreSQL server port')
    parser.add_argument(
        '--pguser', type=str, default='postgres',
        help='PostgreSQL server user')
    parser.add_argument(
        '--queryfiles', help='comma-separated list of files with queries')
    parser.add_argument(
        'benchmark', help='benchmark(s) to run (omit to run all benchmarks)',
        nargs='*')

    args = parser.parse_args()
    mydir = os.path.dirname(os.path.realpath(__file__))

    concurrency_levels = \
        [int(cl) for cl in sorted(args.concurrency_levels.split(','))]

    if args.queryfiles:
        queryfiles = args.queryfiles.split(',')
    else:
        queryfiles = [
            os.path.join(mydir, f)
            for f in glob.glob(os.path.join(mydir, 'queries', '*.json'))
        ]

    variations = []
    querynames = []
    queries = []

    for concurrency in concurrency_levels:
        for queryfile in sorted(queryfiles):
            queryname = os.path.basename(queryfile)
            querynames.append(queryname)

            with open(os.path.join(mydir, queryfile)) as f:
                querydata = json.load(f)
                queries.append(querydata)

            variations.append({
                'title': '{}, concurrency {}'.format(
                    queryname, concurrency
                ),
                'concurrency': concurrency,
                'queryfile': queryfile,
            })

    if not args.pghost:
        print('Initializing temporary PostgreSQL cluster...')
        pg_cluster = asyncpg.cluster.TempCluster()
        pg_cluster.init(user='postgres')
        pg_cluster.trust_local_connections()
        pg_cluster.start(port='dynamic')

        pghost, pgport = pg_cluster.get_connection_addr()
        pguser = 'postgres'
    else:
        pg_cluster = None
        pghost = args.pghost
        pgport = args.pgport
        pguser = args.pguser

    try:
        benchmarks_data = \
            run_benchmarks(args, variations, pghost, pgport, pguser)
    finally:
        if pg_cluster is not None:
            pg_cluster.stop()
            pg_cluster.destroy()

    if args.save_json or args.save_html:
        plat_info = platform_info()

        report_data = {
            'date': '%Y-%m-%dT%H:%M:%S%z',
            'duration': args.duration,
            'platform': plat_info,
            'concurrency_levels': concurrency_levels,
            'querynames': querynames,
            'queries': queries,
            'benchmarks': benchmarks_data,
        }

    if args.save_json:
        with open(args.save_json, 'w') as f:
            json.dump(report_data, f)

    if args.save_html:
        format_report(report_data, args.save_html)


if __name__ == '__main__':
    main()
